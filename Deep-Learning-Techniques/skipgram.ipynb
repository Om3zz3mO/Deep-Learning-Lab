{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWsEoa6BHgHV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "stopwods=nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3izXEGJCFW_5",
        "outputId": "9be9e172-2405-4722-8e50-e384b8c56f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()"
      ],
      "metadata": {
        "id": "m_Oqz-FaOHQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class word2vec(object):\n",
        "\tdef __init__(self):\n",
        "\t\tself.N = 10\n",
        "\t\tself.X_train = []\n",
        "\t\tself.y_train = []\n",
        "\t\tself.window_size = 2\n",
        "\t\tself.alpha = 0.001\n",
        "\t\tself.words = []\n",
        "\t\tself.word_index = {}\n",
        "\n",
        "\tdef initialize(self,V,data):\n",
        "\t\tself.V = V\n",
        "\t\tself.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
        "\t\tself.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
        "\t\t\n",
        "\t\tself.words = data\n",
        "\t\tfor i in range(len(data)):\n",
        "\t\t\tself.word_index[data[i]] = i\n",
        "\n",
        "\tdef feed_forward(self,X):\n",
        "\t\tself.h = np.dot(self.W.T,X).reshape(self.N,1)\n",
        "\t\tself.u = np.dot(self.W1.T,self.h)\n",
        "\t\t#print(self.u)\n",
        "\t\tself.y = softmax(self.u)\n",
        "\t\treturn self.y\n",
        "\t\t\n",
        "\tdef backpropagate(self,x,t):\n",
        "\t\te = self.y - np.asarray(t).reshape(self.V,1)\n",
        "\t\tdLdW1 = np.dot(self.h,e.T)\n",
        "\t\tX = np.array(x).reshape(self.V,1)\n",
        "\t\tdLdW = np.dot(X, np.dot(self.W1,e).T)\n",
        "\t\tself.W1 = self.W1 - self.alpha*dLdW1\n",
        "\t\tself.W = self.W - self.alpha*dLdW\n",
        "\t\t\n",
        "\tdef train(self,epochs):\n",
        "\t\tfor x in range(1,epochs):\t\n",
        "\t\t\tself.loss = 0\n",
        "\t\t\tfor j in range(len(self.X_train)):\n",
        "\t\t\t\tself.feed_forward(self.X_train[j])\n",
        "\t\t\t\tself.backpropagate(self.X_train[j],self.y_train[j])\n",
        "\t\t\t\tC = 0\n",
        "\t\t\t\tfor m in range(self.V):\n",
        "\t\t\t\t\tif(self.y_train[j][m]):\n",
        "\t\t\t\t\t\tself.loss += -1*self.u[m][0]\n",
        "\t\t\t\t\t\tC += 1\n",
        "\t\t\t\tself.loss += C*np.log(np.sum(np.exp(self.u)))\n",
        "\t\t\tprint(\"epoch \",x, \" loss = \",self.loss)\n",
        "\t\t\tself.alpha *= 1/( (1+self.alpha*x) )\n",
        "\t\t\t\n",
        "\tdef predict(self,word,number_of_predictions):\n",
        "\t\tif word in self.words:\n",
        "\t\t\tindex = self.word_index[word]\n",
        "\t\t\tX = [0 for i in range(self.V)]\n",
        "\t\t\tX[index] = 1\n",
        "\t\t\tprediction = self.feed_forward(X)\n",
        "\t\t\toutput = {}\n",
        "\t\t\tfor i in range(self.V):\n",
        "\t\t\t\toutput[prediction[i][0]] = i\n",
        "\t\t\t\n",
        "\t\t\ttop_context_words = []\n",
        "\t\t\tfor k in sorted(output,reverse=True):\n",
        "\t\t\t\ttop_context_words.append(self.words[output[k]])\n",
        "\t\t\t\tif(len(top_context_words)>=number_of_predictions):\n",
        "\t\t\t\t\tbreak\n",
        "\t\n",
        "\t\t\treturn top_context_words\n",
        "\t\telse:\n",
        "\t\t\tprint(\"Word not found in dictionary\")"
      ],
      "metadata": {
        "id": "9YViN8bbNkeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(corpus):\n",
        "    stop_words = set(stopwords.words('english'))   \n",
        "    training_data = []\n",
        "    sentences = corpus.split(\".\")\n",
        "    for i in range(len(sentences)):\n",
        "        sentences[i] = sentences[i].strip()\n",
        "        sentence = sentences[i].split()\n",
        "        x = [word.strip(string.punctuation) for word in sentence\n",
        "                                     if word not in stop_words]\n",
        "        x = [word.lower() for word in x]\n",
        "        training_data.append(x)\n",
        "    return training_data"
      ],
      "metadata": {
        "id": "XkJ64IEJFuSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_for_training(sentences,w2v):\n",
        "    data = {}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word not in data:\n",
        "                data[word] = 1\n",
        "            else:\n",
        "                data[word] += 1\n",
        "    V = len(data)\n",
        "    data = sorted(list(data.keys()))\n",
        "    vocab = {}\n",
        "    for i in range(len(data)):\n",
        "        vocab[data[i]] = i\n",
        "        \n",
        "    for sentence in sentences:\n",
        "        for i in range(len(sentence)):\n",
        "            center_word = [0 for x in range(V)]\n",
        "            center_word[vocab[sentence[i]]] = 1\n",
        "            context = [0 for x in range(V)]\n",
        "             \n",
        "            for j in range(i-w2v.window_size,i+w2v.window_size):\n",
        "                if i!=j and j>=0 and j<len(sentence):\n",
        "                    context[vocab[sentence[j]]] += 1\n",
        "            w2v.X_train.append(center_word)\n",
        "            w2v.y_train.append(context)\n",
        "    w2v.initialize(V,data)\n",
        "  \n",
        "    return w2v.X_train,w2v.y_train"
      ],
      "metadata": {
        "id": "xyw9oyUtFwzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\n",
        "corpus += \"The quick brown fox jumps over the lazy red dog\"\n",
        "epochs = 100\n",
        "\n",
        "training_data = preprocessing(corpus)\n",
        "w2v = word2vec()\n",
        "\n",
        "prepare_data_for_training(training_data,w2v)\n",
        "w2v.train(epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzLKXJbCFyu2",
        "outputId": "f77a3e7a-d665-4cf6-efe5-8605fe7353ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1  loss =  48.6543201385857\n",
            "epoch  2  loss =  48.586237592520376\n",
            "epoch  3  loss =  48.51848326337098\n",
            "epoch  4  loss =  48.451121564047206\n",
            "epoch  5  loss =  48.38421512163501\n",
            "epoch  6  loss =  48.31782443349951\n",
            "epoch  7  loss =  48.25200755053827\n",
            "epoch  8  loss =  48.186819791540955\n",
            "epoch  9  loss =  48.122313491858236\n",
            "epoch  10  loss =  48.058537788774345\n",
            "epoch  11  loss =  47.99553844514051\n",
            "epoch  12  loss =  47.933357711993786\n",
            "epoch  13  loss =  47.87203423008319\n",
            "epoch  14  loss =  47.81160296947787\n",
            "epoch  15  loss =  47.752095205760256\n",
            "epoch  16  loss =  47.69353853072603\n",
            "epoch  17  loss =  47.63595689503303\n",
            "epoch  18  loss =  47.57937067986761\n",
            "epoch  19  loss =  47.52379679442922\n",
            "epoch  20  loss =  47.46924879587189\n",
            "epoch  21  loss =  47.4157370282712\n",
            "epoch  22  loss =  47.3632687772065\n",
            "epoch  23  loss =  47.31184843663959\n",
            "epoch  24  loss =  47.261477684928316\n",
            "epoch  25  loss =  47.21215566701837\n",
            "epoch  26  loss =  47.16387918009903\n",
            "epoch  27  loss =  47.11664286027563\n",
            "epoch  28  loss =  47.07043936809311\n",
            "epoch  29  loss =  47.025259571031214\n",
            "epoch  30  loss =  46.9810927213746\n",
            "epoch  31  loss =  46.93792662813434\n",
            "epoch  32  loss =  46.895747821955\n",
            "epoch  33  loss =  46.85454171218157\n",
            "epoch  34  loss =  46.81429273547844\n",
            "epoch  35  loss =  46.77498449558849\n",
            "epoch  36  loss =  46.736599893993116\n",
            "epoch  37  loss =  46.69912125138368\n",
            "epoch  38  loss =  46.6625304199827\n",
            "epoch  39  loss =  46.62680888685949\n",
            "epoch  40  loss =  46.591937868472684\n",
            "epoch  41  loss =  46.55789839674156\n",
            "epoch  42  loss =  46.524671397001406\n",
            "epoch  43  loss =  46.4922377582389\n",
            "epoch  44  loss =  46.46057839602944\n",
            "epoch  45  loss =  46.42967430861664\n",
            "epoch  46  loss =  46.399506626581235\n",
            "epoch  47  loss =  46.37005665654752\n",
            "epoch  48  loss =  46.34130591937001\n",
            "epoch  49  loss =  46.31323618323209\n",
            "epoch  50  loss =  46.28582949207487\n",
            "epoch  51  loss =  46.259068189756725\n",
            "epoch  52  loss =  46.23293494032499\n",
            "epoch  53  loss =  46.207412744760994\n",
            "epoch  54  loss =  46.18248495453739\n",
            "epoch  55  loss =  46.158135282305864\n",
            "epoch  56  loss =  46.134347810010354\n",
            "epoch  57  loss =  46.11110699470082\n",
            "epoch  58  loss =  46.08839767229998\n",
            "epoch  59  loss =  46.06620505955688\n",
            "epoch  60  loss =  46.04451475440038\n",
            "epoch  61  loss =  46.023312734888464\n",
            "epoch  62  loss =  46.00258535693102\n",
            "epoch  63  loss =  45.9823193509479\n",
            "epoch  64  loss =  45.962501817609024\n",
            "epoch  65  loss =  45.943120222788494\n",
            "epoch  66  loss =  45.92416239185213\n",
            "epoch  67  loss =  45.90561650338572\n",
            "epoch  68  loss =  45.88747108245948\n",
            "epoch  69  loss =  45.86971499351508\n",
            "epoch  70  loss =  45.852337432950954\n",
            "epoch  71  loss =  45.835327921474224\n",
            "epoch  72  loss =  45.81867629627914\n",
            "epoch  73  loss =  45.802372703104886\n",
            "epoch  74  loss =  45.7864075882195\n",
            "epoch  75  loss =  45.77077169037079\n",
            "epoch  76  loss =  45.75545603273979\n",
            "epoch  77  loss =  45.74045191492738\n",
            "epoch  78  loss =  45.72575090500139\n",
            "epoch  79  loss =  45.71134483162648\n",
            "epoch  80  loss =  45.69722577629658\n",
            "epoch  81  loss =  45.68338606568648\n",
            "epoch  82  loss =  45.66981826413614\n",
            "epoch  83  loss =  45.656515166279334\n",
            "epoch  84  loss =  45.64346978982587\n",
            "epoch  85  loss =  45.6306753685046\n",
            "epoch  86  loss =  45.61812534517328\n",
            "epoch  87  loss =  45.60581336509941\n",
            "epoch  88  loss =  45.593733269414926\n",
            "epoch  89  loss =  45.581879088747\n",
            "epoch  90  loss =  45.570245037025316\n",
            "epoch  91  loss =  45.55882550546655\n",
            "epoch  92  loss =  45.547615056734706\n",
            "epoch  93  loss =  45.53660841927648\n",
            "epoch  94  loss =  45.52580048182971\n",
            "epoch  95  loss =  45.515186288102335\n",
            "epoch  96  loss =  45.50476103161957\n",
            "epoch  97  loss =  45.49452005073618\n",
            "epoch  98  loss =  45.48445882381031\n",
            "epoch  99  loss =  45.47457296453578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w2v.predict(\"jumps\",6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCiUYh5YF9Zb",
        "outputId": "69e28cd2-808f-4b77-cba8-f49358c68ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['brown', 'red', 'lazy', 'fox', 'jumps', 'quick']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w2v.predict(\"cat\",5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAppKD9IF-Xb",
        "outputId": "c2842385-619c-4eee-8a67-256e24bc4626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word not found in dictionary\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}